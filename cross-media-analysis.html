<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <link rel="icon" href="favicon.ico">
    <title>Homepage of Jingkuan Song, UESTC</title>
    <script defer="" src="js/chunk-vendors.js"></script>
    <script defer="" src="js/app.js"></script>
    <style type="text/css">
        body {
            height: 100%;
            width: 100%;
            font-family: "OpenSans", Helvetica, Arial, sans-serif;
            font-weight: 400;
            font-size: 15px;
            color: #555;
            line-height: 22px;
        }
    </style>
    <style type="text/css">
        .sc-item[data-v-0ff541a6] {
            margin-bottom: 10px;
        }
    </style>
    <style type="text/css">
        .sl-container[data-v-5cd10541] {
            box-shadow: 0 6px 5px -7px gray;
            padding: 30px 1%;
            color: #555;
        }
    </style>
    <style type="text/css">
        .lc-container[data-v-62a49cd2] {
            box-shadow: 0 6px 5px -7px gray;
        }

        .lc-box[data-v-62a49cd2] {
            padding: 10px 0 20px 0;
            border-bottom: solid 1px #EEE;
            display: flex;
        }

        .lc-cover[data-v-62a49cd2] {
            width: 150px;
            height: auto;
            margin-right: 50px;
        }

        .lc-title[data-v-62a49cd2] {
            font-weight: bold;
        }

        .lc-info[data-v-62a49cd2] {
            margin: 10px 0 10px 0;
            font-style: italic;
        }

        .lc-publish[data-v-62a49cd2] {
            color: #500;
            margin-right: 20px;
        }

        .lc-link-box[data-v-62a49cd2] {
            display: flex;
        }

        .lc-link[data-v-62a49cd2] {
            color: #0088CC;
            margin: 0 2px 0 5px;
        }

        img[data-v-62a49cd2] {
            width: 100%;
            height: auto;
            border-radius: 5px;
        }

        h2[data-v-62a49cd2] {
            font-size: 30px;
            color: #970024;
        }
    </style>
    <style type="text/css">
        .l-link[data-v-702382f0] {
            color: #0088CC;
        }

        .sep[data-v-702382f0] {
            margin: 0 2px 0 0;
        }

        p[data-v-702382f0] {
            float: left;
            word-break: break-all;
            margin: 0 0 10px;
        }
    </style>
    <style type="text/css">
        .container[data-v-0c6f0290] {
            margin: 40px auto;
            max-width: 1000px;
            font-size: 1em;
        }

        .info-box[data-v-0c6f0290] {
            width: 100%;
            display: flex;
            box-shadow: 0 6px 5px -7px gray;
            padding: 10px 1%;
        }

        .avatar-box[data-v-0c6f0290] {
            width: 35%;
            height: auto;
            margin-top: 15px;
            padding: 0 1.5%;
        }

        .info[data-v-0c6f0290] {
            width: 60%;
            padding: 0 1.5%;
        }

        .pad[data-v-0c6f0290] {
            width: 5%;
        }

        .job-title[data-v-0c6f0290] {
            font-style: italic;
            margin: 0 0 10px;
        }

        img[data-v-0c6f0290] {
            width: 100%;
            height: auto;
            border-radius: 5px;
        }

        h1[data-v-0c6f0290] {
            font-size: 36px;
            color: darkblue;
            margin: 0 0 10px;
        }
    </style>
    <style type="text/css">
        li[data-v-1eaa13bc] {
            padding-left: 20px;
        }
    </style>
    <style type="text/css">
        h1[data-v-3a4951d4] {
            font-size: 30px;
            color: darkblue;
            padding-bottom: 10px;
            border-bottom: solid 1px #EEE;
        }
    </style>
    <style type="text/css">
        .container[data-v-0a2d6c68] {
            margin: 40px auto;
            max-width: 1000px;
            font-size: 1em;
        }
    </style>
    <style type="text/css">
        .container[data-v-dcb24c48] {
            margin: 40px auto;
            max-width: 1000px;
            font-size: 1em;
        }
    </style>
    <style type="text/css">
        .container[data-v-247c96ae] {
            margin: 40px auto;
            max-width: 1000px;
            font-size: 1em;
        }
    </style>
    <style type="text/css">
        .container[data-v-606756d4] {
            margin: 40px auto;
            max-width: 1000px;
            font-size: 1em;
        }
    </style>
    <style type="text/css">
        ._th-container ._th-item {
            margin-bottom: 3px;
            position: relative;
            width: 0;
            height: 0;
            cursor: pointer;
            opacity: .3;
            background-color: aquamarine;
            border-radius: 100%;
            text-align: center;
            line-height: 30px;
            -webkit-transition: all .35s;
            -o-transition: all .35s;
            transition: all .35s;
            right: 30px
        }

        ._th-container ._th-item,
        ._th-container ._th-click-hover,
        ._th_cover-all-show-times ._th_times {
            -webkit-box-shadow: -3px 4px 12px -5px black;
            box-shadow: -3px 4px 12px -5px black
        }

        ._th-container:hover ._th-item._item-x2 {
            margin-left: 18px;
            width: 40px;
            height: 40px;
            line-height: 40px
        }

        ._th-container:hover ._th-item._item-x-2 {
            margin-left: 17px;
            width: 38px;
            height: 38px;
            line-height: 38px
        }

        ._th-container:hover ._th-item._item-xx2 {
            width: 36px;
            height: 36px;
            margin-left: 16px;
            line-height: 36px
        }

        ._th-container:hover ._th-item._item-xx-2 {
            width: 32px;
            height: 32px;
            line-height: 32px;
            margin-left: 14px
        }

        ._th-container:hover ._th-item._item-reset {
            width: 30px;
            line-height: 30px;
            height: 30px;
            margin-left: 10px
        }

        ._th-click-hover {
            position: relative;
            -webkit-transition: all .5s;
            -o-transition: all .5s;
            transition: all .5s;
            height: 45px;
            width: 45px;
            cursor: pointer;
            opacity: .3;
            border-radius: 100%;
            background-color: aquamarine;
            text-align: center;
            line-height: 45px;
            right: 0
        }

        ._th-container:hover {
            left: -5px
        }

        ._th-container {
            font-size: 12px;
            -webkit-transition: all .5s;
            -o-transition: all .5s;
            transition: all .5s;
            left: -35px;
            top: 20%;
            position: fixed;
            -webkit-box-sizing: border-box;
            box-sizing: border-box;
            z-index: 100000;
            -webkit-user-select: none;
            -moz-user-select: none;
            -ms-user-select: none;
            user-select: none
        }

        ._th-container ._th-item:hover {
            opacity: .8;
            background-color: #5fb492;
            color: aliceblue
        }

        ._th-container ._th-item:active {
            opacity: .9;
            background-color: #1b3a26;
            color: aliceblue
        }

        ._th-container:hover ._th-click-hover {
            opacity: .8
        }

        ._th-container:hover ._th-item {
            opacity: .6;
            right: 0
        }

        ._th-container ._th-click-hover:hover {
            opacity: .8;
            background-color: #5fb492;
            color: aliceblue
        }

        ._th_cover-all-show-times {
            position: fixed;
            top: 0;
            right: 0;
            width: 100%;
            height: 100%;
            z-index: 99999;
            opacity: 1;
            font-weight: 900;
            font-size: 30px;
            color: #4f4f4f;
            background-color: rgba(0, 0, 0, 0.1)
        }

        ._th_cover-all-show-times._th_hidden {
            z-index: -99999;
            opacity: 0;
            -webkit-transition: 1s all;
            -o-transition: 1s all;
            transition: 1s all
        }

        ._th_cover-all-show-times ._th_times {
            width: 300px;
            height: 300px;
            border-radius: 50%;
            background-color: rgba(127, 255, 212, 0.51);
            text-align: center;
            line-height: 300px;
            position: absolute;
            top: 50%;
            right: 50%;
            margin-top: -150px;
            margin-right: -150px
        }
    </style>
</head>

<body>
    <noscript>
        <strong>We're sorry but songjingkuan doesn't work properly without JavaScript enabled. Please enable it to
            continue.</strong>
    </noscript>
    <div id="app" data-v-app="">
        <div id="container">
            <div class="container" data-v-247c96ae="">
                <p data-v-702382f0=""><span class="l-box" data-v-702382f0=""><a href="index.html" class="l-link"
                            data-v-702382f0="">Back</a><span class="sep" data-v-702382f0=""> | </span></span><span
                        class="l-box" data-v-702382f0=""><a href="compact-representation.html" class="l-link"
                            data-v-702382f0="">Compact Representation</a><span class="sep" data-v-702382f0=""> |
                        </span></span><span class="l-box" data-v-702382f0=""><a href="cross-media-analysis.html"
                            class="l-link" data-v-702382f0="">Cross-media Analysis</a><span class="sep"
                            data-v-702382f0=""> | </span></span><span class="l-box" data-v-702382f0=""><a
                            href="ai-safety.html" class="l-link" data-v-702382f0="">Representation Safety</a>
                        <!--v-if-->
                    </span></p>
                <div data-v-702382f0="" style="clear: both;"></div>
                <div class="lc-container" data-v-62a49cd2="" data-v-247c96ae="">
                    <h2 data-v-62a49cd2="">Cross Media Analysis</h2>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia0.82b0f9a4.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">35. Progressive Meta-Learning With Curriculum
                                </div>
                                <div class="lc-info" data-v-62a49cd2="">Ji Zhang, Jingkuan Song, Lianli Gao, Ye Liu,
                                    Heng Tao Shen</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">IEEE Trans. Circuits Syst. Video Technol.
                                        2022</div><a class="lc-link" href="https://ieeexplore.ieee.org/document/9745972"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia4.ff36076f.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">34. Video Question Answering With Prior
                                    Knowledge and Object-Sensitive Learning</div>
                                <div class="lc-info" data-v-62a49cd2="">Pengpeng Zeng, Haonan Zhang, Lianli Gao,
                                    Jingkuan Song, Heng Tao Shen</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">IEEE Trans. Image Process. 2022</div><a
                                        class="lc-link" href="https://ieeexplore.ieee.org/document/9882977/"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia5.ffa43147.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">33. Fine-Grained Predicates Learning for Scene
                                    Graph Generation</div>
                                <div class="lc-info" data-v-62a49cd2="">Xinyu Lyu, Lianli Gao, Yuyu Guo, Zhou Zhao, Hao
                                    Huang, Heng Tao Shen, Jingkuan Song</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">CVPR 2022</div><a class="lc-link"
                                        href="https://arxiv.org/pdf/2204.02597.pdf" data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia6.e205f486.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">32. Towards Open-Vocabulary Scene Graph
                                    Generation with Prompt-Based Finetuning</div>
                                <div class="lc-info" data-v-62a49cd2="">Tao He, Lianli Gao, Jingkuan Song, Yuan-Fang Li
                                </div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ECCV 2022</div><a class="lc-link"
                                        href="https://arxiv.org/pdf/2208.08165v3.pdf" data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia7.fe8fbf22.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">31. S2 Transformer for Image Captioning</div>
                                <div class="lc-info" data-v-62a49cd2="">Pengpeng Zeng, Haonan Zhang, Jingkuan Song,
                                    Lianli Gao</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">IJCAI 2022</div><a class="lc-link"
                                        href="https://www.ijcai.org/proceedings/2022/0224.pdf"
                                        data-v-62a49cd2="">PDF</a><a class="lc-link"
                                        href="https://github.com/zchoi/S2-Transformer" data-v-62a49cd2="">CODE</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia8.167319a9.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">30. DHHN: Dual Hierarchical Hybrid Network for
                                    Weakly-Supervised Audio-Visual Video Parsing</div>
                                <div class="lc-info" data-v-62a49cd2="">Xun Jiang, Xing Xu, Zhiguo Chen, Jingran Zhang,
                                    Jingkuan Song, Fumin Shen, Huimin Lu, Heng Tao Shen</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ACM Multimedia 2022</div><a
                                        class="lc-link" href="https://dl.acm.org/doi/10.1145/3503161.3548309"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia9.af3c1ff7.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">29. Prompting for Multi-Modal Tracking</div>
                                <div class="lc-info" data-v-62a49cd2="">Jinyu Yang, Zhe Li, Feng Zheng, Ales Leonardis,
                                    Jingkuan Song</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ACM Multimedia 2022</div><a
                                        class="lc-link" href="https://arxiv.org/pdf/2207.14571.pdf"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia10.eecead76.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">28. Progressive Tree-Structured Prototype
                                    Network for End-to-End Image Captioning</div>
                                <div class="lc-info" data-v-62a49cd2="">Pengpeng Zeng, Jinkuan Zhu, Jingkuan Song,
                                    Lianli Gao</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ACM Multimedia 2022</div><a
                                        class="lc-link" href="https://arxiv.org/pdf/2211.09460.pdf"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia11.8bc5cdaf.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">27. Dynamic Scene Graph Generation via Temporal
                                    Prior Inference</div>
                                <div class="lc-info" data-v-62a49cd2="">Shuang Wang, Lianli Gao, Xinyu Lyu, Yuyu Guo,
                                    Pengpeng Zeng, Jingkuan Song</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ACM Multimedia 2022</div><a
                                        class="lc-link" href="https://dl.acm.org/doi/10.1145/3503161.3548324"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia2.bf06d34e.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">26. Relation Regularized Scene Graph Generation
                                </div>
                                <div class="lc-info" data-v-62a49cd2="">Yuyu Guo, Lianli Gao, Jingkuan Song, Peng Wang,
                                    Nicu Sebe, Heng Tao Shen, Xuelong Li</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">IEEE Trans. Cybern. 2021</div><a
                                        class="lc-link" href="https://ieeexplore.ieee.org/document/9376912"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia3.b837d60b.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">25. Hierarchical Representation Network With
                                    Auxiliary Tasks for Video Captioning and Video Question Answering</div>
                                <div class="lc-info" data-v-62a49cd2="">Lianli Gao, Yu Lei, Pengpeng Zeng, Jingkuan
                                    Song, Meng Wang, Heng Tao Shen</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">IEEE Trans. Image Process. 2021</div><a
                                        class="lc-link" href="https://ieeexplore.ieee.org/document/9592722"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia12.cd548304.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">24. GuessWhich? Visual dialog with attentive
                                    memory network</div>
                                <div class="lc-info" data-v-62a49cd2="">Lei Zhao, Xinyu Lyu, Jingkuan Song, Lianli Gao
                                </div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">Pattern Recognit. 2021</div><a
                                        class="lc-link"
                                        href="https://www.sciencedirect.com/science/article/pii/S0031320321000108"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia13.548d7bc9.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">23. Conceptual and Syntactical Cross-modal
                                    Alignment with Cross-level Consistency for Image-Text Matching</div>
                                <div class="lc-info" data-v-62a49cd2="">Pengpeng Zeng, Lianli Gao, Xinyu Lyu, Shuaiqi
                                    Jing, Jingkuan Song</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ACM Multimedia 2021</div><a
                                        class="lc-link" href="https://dl.acm.org/doi/10.1145/3474085.3475380"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia15.053d94ce.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">22. Rich Visual Knowledge-Based Augmentation
                                    Network for Visual Question Answering</div>
                                <div class="lc-info" data-v-62a49cd2="">Liyang Zhang, Shuaicheng Liu, Donghao Liu,
                                    Pengpeng Zeng, Xiangpeng Li, Jingkuan Song, Lianli Gao</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">IEEE Trans. Neural Networks Learn. Syst.
                                        2021</div><a class="lc-link" href="https://ieeexplore.ieee.org/document/9199272"
                                        data-v-62a49cd2="">PDF</a><a class="lc-link"
                                        href="https://github.com/yyyanglz/KAN" data-v-62a49cd2="">CODE</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia16.515b9f07.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">21. Exploiting Scene Graphs for Human-Object
                                    Interaction Detection</div>
                                <div class="lc-info" data-v-62a49cd2="">Tao He, Lianli Gao, Jingkuan Song, Yuan-Fang Li
                                </div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ICCV 2021</div><a class="lc-link"
                                        href="https://arxiv.org/pdf/2108.08584.pdf" data-v-62a49cd2="">PDF</a><a
                                        class="lc-link" href="https://github.com/ht014/%20SG2HOI"
                                        data-v-62a49cd2="">CODE</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia17.bc5d4754.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">20. From General to Specific: Informative Scene
                                    Graph Generation via Balance Adjustment</div>
                                <div class="lc-info" data-v-62a49cd2="">Yuyu Guo, Lianli Gao, Xuanhan Wang, Yuxuan Hu,
                                    Xing Xu, Xu Lu, Heng Tao Shen, Jingkuan Song</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ICCV 2021</div><a class="lc-link"
                                        href="https://openaccess.thecvf.com/content/ICCV2021/html/Guo_From_General_to_Specific_Informative_Scene_Graph_Generation_via_Balance_ICCV_2021_paper.html"
                                        data-v-62a49cd2="">PDF</a><a class="lc-link"
                                        href="https://github.com/ZhuGeKongKong/SGG-G2S" data-v-62a49cd2="">CODE</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia18.520ec4cd.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">19. A System for Interactive and Intelligent AD
                                    Auxiliary Screening</div>
                                <div class="lc-info" data-v-62a49cd2="">Sen Yang, Qike Zhao, Lanxin Miao, Min Chen,
                                    Lianli Gao, Jingkuan Song, Weidong Le</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ACM Multimedia 2021</div><a
                                        class="lc-link" href="https://dl.acm.org/doi/abs/10.1145/3474085.3478549"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia19.425a4e5e.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">18. Exploring Contextual-Aware Representation
                                    and Linguistic-Diverse Expression for Visual Dialog</div>
                                <div class="lc-info" data-v-62a49cd2="">Xiangpeng Li, Lianli Gao, Lei Zhao, Jingkuan
                                    Song</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ACM Multimedia 2021</div><a
                                        class="lc-link" href="https://dl.acm.org/doi/10.1145/3474085.3475360"
                                        data-v-62a49cd2="">PDF</a><a class="lc-link"
                                        href="https://github.com/lixiangpengcs/CARE" data-v-62a49cd2="">CODE</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia1.a3dfe63c.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">17. Learning Cross-Modal Common Representations
                                    by Private-Shared Subspaces Separation</div>
                                <div class="lc-info" data-v-62a49cd2="">Xing Xu, Kaiyi Lin, Lianli Gao, Huimin Lu, Heng
                                    Tao Shen, Xuelong Li</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">IEEE Trans. Cybern. 2020</div><a
                                        class="lc-link" href="https://ieeexplore.ieee.org/abstract/document/9165187"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia14.fc9f752b.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">16. BATCH: A Scalable Asymmetric Discrete
                                    Cross-Modal Hashing</div>
                                <div class="lc-info" data-v-62a49cd2="">Yongxin Wang, Xin Luo, Liqiang Nie, Jingkuan
                                    Song, Wei Zhang, Xin-Shun Xu</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">IEEE Trans. Knowl. Data Eng. 2020</div><a
                                        class="lc-link" href="https://ieeexplore.ieee.org/document/9001235"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia22.b444c026.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">15. Learning from the Scene and Borrowing from
                                    the Rich: Tackling the Long Tail in Scene Graph Generation</div>
                                <div class="lc-info" data-v-62a49cd2="">Tao He, Lianli Gao, Jingkuan Song, Jianfei Cai,
                                    Yuan-Fang Li</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">IJCAI 2020</div><a class="lc-link"
                                        href="https://www.ijcai.org/proceedings/2020/0082.pdf"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia23.a4396055.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">14. One-shot Scene Graph Generation</div>
                                <div class="lc-info" data-v-62a49cd2="">Yuyu Guo, Jingkuan Song, Lianli Gao, Heng Tao
                                    Shen</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ACM Multimedia 2020</div><a
                                        class="lc-link" href="https://dl.acm.org/doi/abs/10.1145/3394171.3414025"
                                        data-v-62a49cd2="">PDF</a><a class="lc-link"
                                        href="https://github.com/gyy8426/OS-SGG" data-v-62a49cd2="">CODE</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia20.f600a564.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">13. Hierarchical LSTMs with Adaptive Attention
                                    for Visual Captioning</div>
                                <div class="lc-info" data-v-62a49cd2="">Lianli Gao, Xiangpeng Li, Jingkuan Song, Heng
                                    Tao Shen</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">IEEE Trans. Pattern Anal. Mach. Intell.
                                        2019</div><a class="lc-link"
                                        href="https://ieeexplore.ieee.org/abstract/document/8620348"
                                        data-v-62a49cd2="">PDF</a><a class="lc-link"
                                        href="https://github.com/lixiangpengcs/Spatial-Temporal-Adaptive-Attention-for-Video-Captioning"
                                        data-v-62a49cd2="">CODE</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia21.bc756d57.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">12. Ternary Adversarial Networks With
                                    Self-Supervision for Zero-Shot Cross-Modal Retrieval</div>
                                <div class="lc-info" data-v-62a49cd2="">Xing Xu, Huimin Lu, Jingkuan Song, Yang Yang,
                                    Heng Tao Shen, Xuelong Li</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">IEEE Trans. Cybern. 2019</div><a
                                        class="lc-link" href="https://ieeexplore.ieee.org/document/8771379"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia27.a4cc8314.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">11. Deliberate Attention Networks for Image
                                    Captioning</div>
                                <div class="lc-info" data-v-62a49cd2="">Lianli Gao, Kaixuan Fan, Jingkuan Song,
                                    Xianglong Liu, Xing Xu, Heng Tao Shen</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">AAAI 2019</div><a class="lc-link"
                                        href="https://ojs.aaai.org/index.php/AAAI/article/view/4845"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia28.8117ae52.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">10. Beyond RNNs: Positional Self-Attention with
                                    Co-Attention for Video Question Answering</div>
                                <div class="lc-info" data-v-62a49cd2="">Xiangpeng Li, Jingkuan Song, Lianli Gao,
                                    Xianglong Liu, Wenbing Huang, Xiangnan He, Chuang Gan</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">AAAI 2019</div><a class="lc-link"
                                        href="https://ojs.aaai.org/index.php/AAAI/article/view/4887"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia29.1492bc02.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">9. Open-Ended Long-Form Video Question
                                    Answering via Hierarchical Convolutional Self-Attention Networks</div>
                                <div class="lc-info" data-v-62a49cd2="">Zhu Zhang, Zhou Zhao, Zhijie Lin, Jingkuan Song,
                                    Xiaofei He</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">IJCAI 2019</div><a class="lc-link"
                                        href="https://arxiv.org/pdf/1906.12158.pdf" data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia30.0db0ab3e.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">8. Matching Images and Text with Multi-modal
                                    Tensor Fusion and Re-ranking</div>
                                <div class="lc-info" data-v-62a49cd2="">Tan Wang, Xing Xu, Yang Yang, Alan Hanjalic,
                                    Heng Tao Shen, Jingkuan Song</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ACM Multimedia 2019</div><a
                                        class="lc-link" href="https://arxiv.org/pdf/1908.04011.pdf"
                                        data-v-62a49cd2="">PDF</a><a class="lc-link"
                                        href="https://github.com/Wangt-CN/MTFN-RR-PyTorch-Code"
                                        data-v-62a49cd2="">CODE</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia24.fd66a8f8.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">7. From Deterministic to Generative: Multimodal
                                    Stochastic RNNs for Video Captioning</div>
                                <div class="lc-info" data-v-62a49cd2="">Jingkuan Song, Yuyu Guo, Lianli Gao, Xuelong Li,
                                    Alan Hanjalic, Heng Tao Shen</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">IEEE Trans. Neural Networks Learn. Syst.
                                        2018</div><a class="lc-link"
                                        href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8438512"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia25.4487b233.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">6. Structured Two-Stream Attention Network for
                                    Video Question Answering</div>
                                <div class="lc-info" data-v-62a49cd2="">Lianli Gao, Pengpeng Zeng, Jingkuan Song,
                                    Yuan-Fang Li, Wu Liu, Tao Mei, Heng Tao Shen</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">AAAI 2019</div><a class="lc-link"
                                        href="https://ojs.aaai.org/index.php/AAAI/article/view/4602"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia26.a7775ce2.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">5. Perceptual Pyramid Adversarial Networks for
                                    Text-to-Image Synthesis</div>
                                <div class="lc-info" data-v-62a49cd2="">Lianli Gao, Daiyuan Chen, Jingkuan Song, Xing
                                    Xu, Dongxiang Zhang, Heng Tao Shen</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">AAAI 2019</div><a class="lc-link"
                                        href="https://ojs.aaai.org/index.php/AAAI/article/view/4844"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia31.27cfab1e.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">4. Learnable Aggregating Net with Diversity
                                    Learning for Video Question Answering</div>
                                <div class="lc-info" data-v-62a49cd2="">Xiangpeng Li, Lianli Gao, Xuanhan Wang, Wu Liu,
                                    Xing Xu, Heng Tao Shen, Jingkuan Song</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ACM Multimedia 2019</div><a
                                        class="lc-link" href="https://dl.acm.org/doi/abs/10.1145/3343031.3350971"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia32.64ff82f9.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">3. From Pixels to Objects: Cubic Visual
                                    Attention for Visual Question Answering</div>
                                <div class="lc-info" data-v-62a49cd2="">Jingkuan Song, Pengpeng Zeng, Lianli Gao, Heng
                                    Tao Shen</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">IJCAI 2018</div><a class="lc-link"
                                        href="https://www.ijcai.org/proceedings/2018/0126.pdf"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia33.c4a9a6a2.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">2. Modal-adversarial Semantic Learning Network
                                    for Extendable Cross-modal Retrieval</div>
                                <div class="lc-info" data-v-62a49cd2="">Xing Xu, Jingkuan Song, Huimin Lu, Yang Yang,
                                    Fumin Shen, Zi Huang</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ICMR 2018</div><a class="lc-link"
                                        href="https://dl.acm.org/doi/abs/10.1145/3206025.3206033"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                    <div data-v-62a49cd2="">
                        <div class="lc-box" data-v-62a49cd2="">
                            <div class="lc-cover" data-v-62a49cd2=""><img src="img/crossmedia34.9a49fad0.png"
                                    data-v-62a49cd2=""></div>
                            <div data-v-62a49cd2="">
                                <div class="lc-title" data-v-62a49cd2="">1. Examine before You Answer: Multi-task
                                    Learning with Adaptive-attentions for Multiple-choice VQA</div>
                                <div class="lc-info" data-v-62a49cd2="">Lianli Gao, Pengpeng Zeng, Jingkuan Song,
                                    Xianglong Liu, Heng Tao Shen</div>
                                <div class="lc-link-box" data-v-62a49cd2="">
                                    <div class="lc-publish" data-v-62a49cd2="">ACM Multimedia 2018</div><a
                                        class="lc-link" href="https://dl.acm.org/doi/10.1145/3240508.3240687"
                                        data-v-62a49cd2="">PDF</a>
                                    <!--v-if-->
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

            </div>
        </div>
    </div>
    <!-- built files will be auto injected -->


    <div>
        <div class="_th-container">
            <div class="_th-click-hover _item-input">
                x1.00
            </div>
            <div class="_th-item _item-x2">&gt;</div>
            <div class="_th-item _item-x-2">&lt;</div>
            <div class="_th-item _item-xx2">&gt;&gt;</div>
            <div class="_th-item _item-xx-2">&lt;&lt;</div>
            <div class="_th-item _item-reset">O</div>
        </div>
        <div class="_th_cover-all-show-times _th_hidden">
            <div class="_th_times">x1.00</div>
        </div>
    </div>
</body>

</html>